# HPCCasoEstudio3 - Project Summary

## ğŸ“‹ Overview

**Project**: Distributed Matrix Multiplication using MPI on AWS Cluster  
**Course**: High Performance Computing  
**Institution**: Universidad TecnolÃ³gica de Pereira  
**Objective**: Implement and benchmark parallel matrix multiplication using Message Passing Interface across a distributed cluster

## ğŸ—ï¸ Architecture

### Cluster Configuration
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AWS Cluster                       â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚  Head Node   â”‚  â”‚   Worker 1   â”‚  â”‚   Worker 2   â”‚
â”‚  â”‚  (Master)    â”‚  â”‚              â”‚  â”‚              â”‚
â”‚  â”‚ 18.224.x.x   â”‚  â”‚ 172.31.x.x   â”‚  â”‚ 172.31.x.x   â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚  â”‚  2 slots     â”‚  â”‚  2 slots     â”‚  â”‚  2 slots     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚       t3.micro          t3.micro          t3.micro
â”‚    1 vCPU, 1GB       1 vCPU, 1GB       1 vCPU, 1GB
â”‚                                                     â”‚
â”‚              Total: 6 MPI Processes                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Distribution Strategy
```
Matrix A (nÃ—n)          Matrix B (nÃ—n)          Matrix C (nÃ—n)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Rows 0-1  â”‚ â”€â”      â”‚             â”‚         â”‚   Rows 0-1  â”‚ â—„â”€â”
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚      â”‚             â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚   Rows 2-3  â”‚ â”€â”¼â”€â”€â–º   â”‚   Complete  â”‚  â”€â”€â–º    â”‚   Rows 2-3  â”‚ â—„â”€â”¤
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚      â”‚   Matrix    â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚   Rows 4-5  â”‚ â”€â”˜      â”‚             â”‚         â”‚   Rows 4-5  â”‚ â—„â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Scattered to           Broadcast to            Gathered from
   6 processes            all processes           6 processes
```

## ğŸ“ Project Structure

```
HPCCasoEstudio3/
â”œâ”€â”€ ğŸ“„ README.md                          # Main documentation
â”œâ”€â”€ ğŸ“„ Makefile                           # Build system
â”œâ”€â”€ ğŸ“„ hostfile                           # MPI cluster config
â”‚
â”œâ”€â”€ ğŸ“‚ src/                               # Source implementations
â”‚   â”œâ”€â”€ matrix_mpi_sequential.c          # [1] Baseline (rank 0 only)
â”‚   â”œâ”€â”€ matrix_mpi_rowwise.c             # [2] Row distribution
â”‚   â”œâ”€â”€ matrix_mpi_broadcast.c           # [3] Optimized broadcast
â”‚   â””â”€â”€ matrix_mpi_nonblocking.c         # [4] Non-blocking I/O
â”‚
â”œâ”€â”€ ğŸ“‚ bin/                               # Compiled binaries
â”‚   â””â”€â”€ matrix_mpi_*                     # (generated by make)
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ ğŸ”§ deploy.sh                     # Deploy to cluster
â”‚   â”œâ”€â”€ ğŸ”§ run_benchmarks.sh             # Execute benchmarks
â”‚   â”œâ”€â”€ ğŸ”§ test_quick.sh                 # Quick local test
â”‚   â””â”€â”€ ğŸ“Š analyze_results.py            # Analysis & plots
â”‚
â”œâ”€â”€ ğŸ“‚ results/                           # Benchmark outputs
â”‚   â”œâ”€â”€ benchmarks.csv                   # Raw timing data
â”‚   â”œâ”€â”€ processed_results.csv            # Computed metrics
â”‚   â”œâ”€â”€ summary_report.txt               # Statistical summary
â”‚   â””â”€â”€ *.png                            # Visualization plots
â”‚
â””â”€â”€ ğŸ“‚ docs/
    â”œâ”€â”€ ğŸ“– AWS_CLUSTER_GUIDE.md          # Deployment guide
    â””â”€â”€ ğŸ“– TECHNICAL_DETAILS.md          # Algorithm analysis

Total: 4 implementations + 4 scripts + 5 documents
```

## ğŸš€ Implementation Details

### 1ï¸âƒ£ Sequential Baseline (`matrix_mpi_sequential.c`)
**Purpose**: Measure MPI framework overhead
- âœ… Uses MPI infrastructure but no parallelization
- âœ… Only rank 0 computes entire result
- âœ… Baseline for calculating speedup

**Key Features**:
- Simple reference implementation
- Measures pure MPI overhead
- Used as Tâ‚ in speedup calculations

### 2ï¸âƒ£ Row-wise Distribution (`matrix_mpi_rowwise.c`)
**Purpose**: Classic master-worker pattern
- âœ… `MPI_Scatter`: Distribute rows of A
- âœ… `MPI_Bcast`: Broadcast entire B to all
- âœ… `MPI_Gather`: Collect result rows
- âœ… Measures computation vs communication time

**Complexity**:
- Computation: O(nÂ³/p)
- Communication: O(nÂ² + nÂ²/p) = O(nÂ²)

### 3ï¸âƒ£ Broadcast Optimized (`matrix_mpi_broadcast.c`)
**Purpose**: Enhanced monitoring and statistics
- âœ… Same algorithm as row-wise
- âœ… Detailed timing breakdown per process
- âœ… Load balance metrics
- âœ… Communication overhead percentage
- âœ… Uses `MPI_Reduce` to collect statistics

**Additional Metrics**:
```
Load Balance = (min_compute_time / max_compute_time) Ã— 100%
Comm Overhead = (max_comm_time / total_time) Ã— 100%
```

### 4ï¸âƒ£ Non-blocking Communication (`matrix_mpi_nonblocking.c`)
**Purpose**: Overlap communication with computation
- âœ… `MPI_Isend/MPI_Irecv`: Non-blocking operations
- âœ… Potential to hide latency
- âœ… Manual request management
- âœ… Calculates overlap efficiency

**Overlap Strategy**:
```
Traditional:  |---Comm---|---Compute---|
Non-blocking: |---Comm---|
                 |---Compute---|
              â””â”€ overlap â”€â”˜
```

## ğŸ“Š Benchmarking Plan

### Test Matrix
| Matrix Size | Processes | Implementations | Total Tests |
|-------------|-----------|-----------------|-------------|
| 512Ã—512     | 2, 4, 6   | 4               | 12          |
| 1024Ã—1024   | 2, 4, 6   | 4               | 12          |
| 2048Ã—2048   | 2, 4, 6   | 4               | 12          |
| **Total**   |           |                 | **36**      |

### Metrics Collected

**Primary Metrics**:
- â±ï¸ Execution time (wall clock)
- ğŸš€ Speedup: `S(p) = T_baseline / T_p`
- ğŸ“ˆ Efficiency: `E(p) = S(p) / p Ã— 100%`

**Secondary Metrics** (broadcast & nonblocking):
- ğŸ’¬ Communication time
- ğŸ”¢ Computation time
- âš–ï¸ Load balance
- ğŸ“¡ Communication overhead

**Expected Results**:
- Ideal speedup: 6x (100% efficiency)
- Realistic speedup: 3-5x (50-83% efficiency)
- Best for large matrices (n â‰¥ 1024)

## ğŸ“ˆ Visualization Outputs

Analysis script generates:
1. **speedup_comparison.png** - Speedup curves by implementation
2. **efficiency_comparison.png** - Parallel efficiency trends
3. **execution_times.png** - Bar chart of all configurations
4. **speedup_heatmap.png** - Heatmap grid of speedup values
5. **summary_report.txt** - Statistical text summary

## ğŸ”§ Workflow

### Development (Local)
```bash
# 1. Compile
make clean && make

# 2. Quick test locally
./scripts/test_quick.sh

# 3. Verify implementations
mpirun -np 2 ./bin/matrix_mpi_sequential 256
mpirun -np 4 ./bin/matrix_mpi_rowwise 512
```

### Deployment (AWS Cluster)
```bash
# 1. Transfer to head node
tar -czf HPCCasoEstudio3.tar.gz HPCCasoEstudio3/
scp HPCCasoEstudio3.tar.gz ubuntu@18.224.187.40:~/

# 2. SSH to head node
ssh ubuntu@18.224.187.40
tar -xzf HPCCasoEstudio3.tar.gz
cd HPCCasoEstudio3

# 3. Compile on cluster
make clean && make

# 4. Deploy to workers
./scripts/deploy.sh

# 5. Verify connectivity
mpirun --hostfile hostfile -np 6 hostname

# 6. Test single run
mpirun --hostfile hostfile -np 6 ./bin/matrix_mpi_rowwise 512
```

### Benchmarking
```bash
# Full benchmark suite (takes 10-15 minutes)
./scripts/run_benchmarks.sh

# Results saved to results/benchmarks.csv
```

### Analysis
```bash
# Option 1: Analyze on cluster (if Python available)
python3 scripts/analyze_results.py

# Option 2: Download to local machine
scp -r ubuntu@18.224.187.40:~/HPCCasoEstudio3/results ./results_aws
cd results_aws
python3 ../scripts/analyze_results.py
```

## ğŸ¯ Learning Objectives Achieved

âœ… **MPI Programming**:
- Collective communications (Scatter, Bcast, Gather)
- Non-blocking point-to-point (Isend, Irecv)
- Process management (rank, size, wait)
- Timing and synchronization

âœ… **Distributed Computing**:
- Master-worker pattern
- Data decomposition strategies
- Load balancing considerations
- Communication optimization

âœ… **Performance Analysis**:
- Speedup and efficiency metrics
- Communication vs computation trade-offs
- Scalability assessment
- Bottleneck identification

âœ… **Cloud Infrastructure**:
- AWS EC2 cluster management
- SSH configuration
- Network topology understanding
- Resource constraints (1GB RAM)

## ğŸ“ Theoretical Foundation

### Amdahl's Law
```
Speedup(p) = 1 / (s + (1-s)/p)
```
Where `s` = serial fraction

For our case:
```
T_parallel = T_compute/p + T_comm + T_overhead
```

### Communication Complexity
- Scatter/Gather: O(nÂ²/p) per process
- Broadcast: O(log p Ã— nÂ²) using tree algorithm
- Total: **O(nÂ²)** dominates for small p

### Computation vs Communication Ratio
```
ratio = (nÂ³/p) / (nÂ² Ã— log p)
      = n / (p Ã— log p)
```

**Implication**: Larger matrices â†’ better speedup
- n=512: ratio â‰ˆ 85 (moderate)
- n=1024: ratio â‰ˆ 170 (good)
- n=2048: ratio â‰ˆ 341 (excellent)

## ğŸ”¬ Technical Specifications

**Language**: C (C99 standard)  
**MPI Standard**: MPI-3.0  
**Compiler**: mpicc (wrapper for gcc/clang)  
**Optimization**: `-O2` flag  
**Data Type**: `double` (8 bytes per element)  
**Matrix Storage**: Row-major (C standard)

**Memory per Process** (n=2048, p=6):
- A_local: ~5.6 MB
- B: ~33.5 MB
- C_local: ~5.6 MB
- **Total**: ~45 MB âœ… Safe for 1GB RAM

## ğŸ“š Documentation

| Document | Purpose | Pages |
|----------|---------|-------|
| README.md | Main overview & usage | 1-2 |
| AWS_CLUSTER_GUIDE.md | Deployment instructions | 3-4 |
| TECHNICAL_DETAILS.md | Algorithm analysis | 5-6 |
| Code comments | Implementation details | Inline |

## ğŸ“ Key Takeaways

1. **Communication is expensive**: Even with 6 processes, communication overhead limits speedup to 3-5x
2. **Matrix size matters**: Larger matrices (n â‰¥ 1024) show better parallel efficiency
3. **Collective ops are optimized**: `MPI_Bcast` uses logarithmic tree, not linear broadcast
4. **Load balance is critical**: All processes must finish at same time for optimal performance
5. **Non-blocking helps marginally**: Overlap potential limited by computation-heavy workload

## ğŸš¦ Status

| Component | Status | Notes |
|-----------|--------|-------|
| âœ… Implementation | Complete | 4 versions coded |
| âœ… Compilation | Ready | Makefile configured |
| âœ… Scripts | Ready | Deploy, benchmark, analyze |
| âœ… Documentation | Complete | 3 detailed guides |
| â³ Benchmarking | Pending | Execute on cluster |
| â³ Analysis | Pending | After benchmark data |
| â³ Report | Pending | After results review |

## ğŸ¯ Next Steps

1. âœ… **Code Complete** - All implementations ready
2. â³ **Deploy to Cluster** - Transfer and compile on AWS
3. â³ **Run Benchmarks** - Execute 36 test configurations
4. â³ **Analyze Results** - Generate plots and statistics
5. â³ **Document Findings** - Write technical report
6. â³ **Present Results** - Prepare presentation

---

**Project Started**: Today  
**Expected Completion**: After cluster benchmarking  
**Total Lines of Code**: ~1200 (C) + ~400 (Python) + ~300 (Bash)  
**Total Files**: 13 code/script files + 5 documentation files

**Ready for cluster deployment!** ğŸš€
