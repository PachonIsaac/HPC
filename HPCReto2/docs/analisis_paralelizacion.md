# AnÃ¡lisis de ParalelizaciÃ³n - Algoritmos Monte Carlo

## ğŸ“Š IntroducciÃ³n

Este documento analiza las opciones de paralelizaciÃ³n con OpenMP para los algoritmos Monte Carlo de estimaciÃ³n de PI: **Buffon's Needle** y **Dartboard**.

---

## ğŸ¯ Algoritmo 1: Buffon's Needle (Monte Carlo Needles)

### DescripciÃ³n del Algoritmo Serial
```c
for (long i = 0; i < needles; i++) {
    double x = ((double)rand() / RAND_MAX) * (DIST / 2);
    double theta = ((double)rand() / RAND_MAX) * M_PI;
    double reach = (LENGTH / 2) * sin(theta);
    if (x <= reach) hits++;
}
```

### AnÃ¡lisis de Dependencias

#### âœ… **CaracterÃ­sticas Paralelizables**
1. **Iteraciones independientes**: Cada lanzamiento de aguja es independiente
2. **No hay dependencias de datos**: Cada iteraciÃ³n trabaja con variables locales
3. **OperaciÃ³n de reducciÃ³n simple**: Solo necesitamos sumar `hits`

#### âš ï¸ **DesafÃ­os**
1. **Generador aleatorio thread-unsafe**: `rand()` no es thread-safe
2. **Variable compartida `hits`**: Requiere sincronizaciÃ³n
3. **Operaciones matemÃ¡ticas costosas**: `sin()` es computacionalmente cara

### Estrategias de ParalelizaciÃ³n con OpenMP

#### **OpciÃ³n 1: Parallel For BÃ¡sico**
```c
#pragma omp parallel for reduction(+:hits)
for (long i = 0; i < needles; i++) {
    // Usar rand_r() con semilla thread-local
    double x = ((double)rand_r(&seed) / RAND_MAX) * (DIST / 2);
    double theta = ((double)rand_r(&seed) / RAND_MAX) * M_PI;
    double reach = (LENGTH / 2) * sin(theta);
    if (x <= reach) hits++;
}
```

**Ventajas**:
- Simple y directo
- `reduction(+:hits)` maneja la sincronizaciÃ³n automÃ¡ticamente
- Overhead mÃ­nimo de paralelizaciÃ³n

**Consideraciones**:
- Necesita generador aleatorio thread-safe (`rand_r()`)
- Cada thread necesita su propia semilla

#### **OpciÃ³n 2: Diferentes Scheduling Policies**

**Static Scheduling** (por defecto):
```c
#pragma omp parallel for schedule(static) reduction(+:hits)
```
- Distribuye iteraciones en chunks de tamaÃ±o fijo
- Bajo overhead
- Mejor para carga balanceada

**Dynamic Scheduling**:
```c
#pragma omp parallel for schedule(dynamic, 1000) reduction(+:hits)
```
- Asigna chunks dinÃ¡micamente
- Mayor overhead
- Ãštil si hay variabilidad en tiempo de ejecuciÃ³n

**Guided Scheduling**:
```c
#pragma omp parallel for schedule(guided) reduction(+:hits)
```
- Chunks adaptativos (grandes al inicio, pequeÃ±os al final)
- Compromiso entre static y dynamic

#### **OpciÃ³n 3: VersiÃ³n Optimizada**

```c
#pragma omp parallel reduction(+:hits)
{
    unsigned int seed = omp_get_thread_num() + time(NULL);
    
    #pragma omp for schedule(static)
    for (long i = 0; i < needles; i++) {
        double x = ((double)rand_r(&seed) / RAND_MAX) * (DIST / 2);
        double theta = ((double)rand_r(&seed) / RAND_MAX) * M_PI;
        double reach = (LENGTH / 2) * sin(theta);
        if (x <= reach) hits++;
    }
}
```

**Optimizaciones adicionales**:
1. **Precalcular constantes**: `LENGTH/2`, `DIST/2`
2. **Lookup tables para sin()**: Si es viable
3. **Usar generadores mÃ¡s rÃ¡pidos**: `drand48_r()` o generadores custom
4. **Padding para evitar false sharing**

---

## ğŸ¯ Algoritmo 2: Dartboard (Monte Carlo Circle)

### DescripciÃ³n del Algoritmo Serial
```c
for (long i = 0; i < darts; i++) {
    double x = ((double)rand() / RAND_MAX) * 2.0 - 1.0;
    double y = ((double)rand() / RAND_MAX) * 2.0 - 1.0;
    if (x * x + y * y <= 1.0) hits++;
}
```

### AnÃ¡lisis de Dependencias

#### âœ… **CaracterÃ­sticas Paralelizables**
1. **Iteraciones completamente independientes**
2. **Operaciones matemÃ¡ticas simples**: Solo multiplicaciÃ³n y suma
3. **Sin funciones transcendentales**: MÃ¡s rÃ¡pido que Needles

#### âš ï¸ **DesafÃ­os**
1. **Mismo problema con `rand()`**: No thread-safe
2. **Variable compartida `hits`**: ReducciÃ³n necesaria

### Estrategias de ParalelizaciÃ³n con OpenMP

#### **OpciÃ³n 1: Parallel For BÃ¡sico**
```c
#pragma omp parallel for reduction(+:hits)
for (long i = 0; i < darts; i++) {
    unsigned int seed = i + omp_get_thread_num();
    double x = ((double)rand_r(&seed) / RAND_MAX) * 2.0 - 1.0;
    double y = ((double)rand_r(&seed) / RAND_MAX) * 2.0 - 1.0;
    if (x * x + y * y <= 1.0) hits++;
}
```

#### **OpciÃ³n 2: VersiÃ³n Optimizada con Menos Operaciones**
```c
#pragma omp parallel reduction(+:hits)
{
    unsigned int seed = omp_get_thread_num() + time(NULL);
    
    #pragma omp for schedule(static)
    for (long i = 0; i < darts; i++) {
        // Evitar multiplicaciÃ³n por 2.0 - 1.0
        double x = ((double)rand_r(&seed) / RAND_MAX);
        double y = ((double)rand_r(&seed) / RAND_MAX);
        x = x + x - 1.0;  // MÃ¡s rÃ¡pido que * 2.0
        y = y + y - 1.0;
        
        // ComparaciÃ³n directa sin sqrt
        if (x * x + y * y <= 1.0) hits++;
    }
}
```

#### **OpciÃ³n 3: Minimizar False Sharing**
```c
#pragma omp parallel
{
    int local_hits = 0;  // Variable local por thread
    unsigned int seed = omp_get_thread_num() + time(NULL);
    
    #pragma omp for schedule(static) nowait
    for (long i = 0; i < darts; i++) {
        double x = ((double)rand_r(&seed) / RAND_MAX) * 2.0 - 1.0;
        double y = ((double)rand_r(&seed) / RAND_MAX) * 2.0 - 1.0;
        if (x * x + y * y <= 1.0) local_hits++;
    }
    
    #pragma omp atomic
    hits += local_hits;  // SincronizaciÃ³n al final
}
```

---

## ğŸ“Š ComparaciÃ³n: Needles vs Dartboard

| Aspecto | Needles | Dartboard |
|---------|---------|-----------|
| **Complejidad por iteraciÃ³n** | Alta (sin, divisiÃ³n) | Baja (multiplicaciÃ³n) |
| **Convergencia a PI** | MÃ¡s lenta | MÃ¡s rÃ¡pida |
| **ParalelizaciÃ³n** | Igual de directa | Igual de directa |
| **Overhead esperado** | Mayor (sin costosa) | Menor (ops simples) |
| **Speedup esperado** | Bueno (trabajo >> overhead) | Excelente (trabajo ligero pero muchas iter) |

---

## ğŸ” Aspectos Importantes de OpenMP

### Variables y ClÃ¡usulas

#### **Private vs Shared**
```c
#pragma omp parallel for private(x, y, theta, reach) shared(needles, hits)
```
- `private`: Cada thread tiene su copia (x, y, theta, reach)
- `shared`: Todos los threads acceden a la misma (needles, hits)
- Por defecto: variables en el scope son compartidas

#### **Reduction**
```c
#pragma omp parallel for reduction(+:hits)
```
- Crea copias privadas de `hits`
- Cada thread acumula en su copia
- Al final, suma todas las copias atÃ³micamente
- **Ideal para nuestro caso**

#### **Scheduling**
- `schedule(static)`: Mejor performance si iteraciones uniformes
- `schedule(static, chunk_size)`: Control manual del chunk
- `schedule(dynamic)`: Balanceo de carga dinÃ¡mico
- `schedule(guided)`: Adaptativo (recomendado probar)

### Evitar False Sharing

**Problema**: MÃºltiples threads modificando variables en la misma lÃ­nea de cache

**SoluciÃ³n**:
```c
struct AlignedCounter {
    long value;
    char padding[64 - sizeof(long)];  // Alinear a cache line
} __attribute__((aligned(64)));
```

### InicializaciÃ³n de Semillas

**Mala prÃ¡ctica**:
```c
unsigned int seed = time(NULL);  // Todos los threads igual semilla
```

**Buena prÃ¡ctica**:
```c
unsigned int seed = omp_get_thread_num() + time(NULL) * (omp_get_thread_num() + 1);
```

---

## ğŸ¯ Plan de ImplementaciÃ³n

### Fase 1: Implementaciones BÃ¡sicas
1. âœ… `pi_needles_serial.c` - Baseline
2. âœ… `pi_dartboard_serial.c` - Baseline
3. `pi_needles_omp_basic.c` - OpenMP bÃ¡sico con reduction
4. `pi_dartboard_omp_basic.c` - OpenMP bÃ¡sico con reduction

### Fase 2: ExploraciÃ³n de Scheduling
5. `pi_needles_omp_static.c` - schedule(static)
6. `pi_needles_omp_dynamic.c` - schedule(dynamic)
7. `pi_needles_omp_guided.c` - schedule(guided)
8. Similar para dartboard (3 versiones)

### Fase 3: Optimizaciones
9. `pi_needles_omp_optimized.c` - Todas las optimizaciones
10. `pi_dartboard_omp_optimized.c` - Todas las optimizaciones

### Optimizaciones a Incluir:
- âœ… Generadores aleatorios thread-safe
- âœ… Evitar false sharing
- âœ… PrecÃ¡lculo de constantes
- âœ… Minimizar llamadas a funciones caras
- âœ… AlineaciÃ³n de datos
- âœ… Flags de compilaciÃ³n agresivos (-O3, -march=native)

---

## ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n

### Speedup
$$S_p = \frac{T_{serial}}{T_{parallel}}$$

### Eficiencia
$$E_p = \frac{S_p}{p} = \frac{T_{serial}}{p \times T_{parallel}}$$

### Overhead
$$O = T_{parallel} - \frac{T_{serial}}{p}$$

### Strong Scaling
- Problema de tamaÃ±o fijo
- Aumentar nÃºmero de procesadores
- Esperado: Speedup lineal (ideal) o sub-lineal (realista)

### Weak Scaling
- Aumentar problema proporcionalmente con procesadores
- Esperado: Tiempo constante (ideal)

---

## ğŸ”¬ Experimentos Sugeridos

1. **VariaciÃ³n de tamaÃ±o**: 10^6, 10^7, 10^8, 10^9 iteraciones
2. **VariaciÃ³n de threads**: 1, 2, 4, 8, 16 threads
3. **Scheduling policies**: static vs dynamic vs guided
4. **Chunk sizes**: Para dynamic/guided
5. **ComparaciÃ³n con Reto1**: pthreads vs fork vs OpenMP

---

## âœ… Conclusiones Preliminares

### Ventajas de OpenMP para estos algoritmos:
1. **Simplicidad**: Directivas simples vs manejo manual de threads
2. **Portabilidad**: EstÃ¡ndar multiplataforma
3. **Eficiencia**: Overhead bajo para trabajo computacional
4. **Flexibilidad**: FÃ¡cil experimentar con diferentes estrategias

### DesafÃ­os:
1. **Random numbers**: Necesita manejo especial thread-safety
2. **Tune scheduling**: Requiere experimentaciÃ³n
3. **AnÃ¡lisis de performance**: Profiling necesario

### Resultados Esperados:
- **Speedup cercano a lineal** para ambos algoritmos (trabajo >> overhead)
- **Dartboard ligeramente mejor** que Needles (operaciones mÃ¡s simples)
- **Static scheduling mejor** para estos casos (iteraciones uniformes)
- **OpenMP comparable o mejor** que pthreads (menos overhead manual)

---
