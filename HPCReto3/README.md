# HPCReto3 - Cellular Automaton Traffic Simulation with MPI

## Project Overview

Implementation and analysis of a 1D cellular automaton traffic flow model with MPI parallelization strategies.

**Course:** High Performance Computing (HPC)  
**Challenge:** Reto 3 - Cellular Automaton Parallelization

## Model Description

### Traffic Rules
- **1D circular road** with N cells (periodic boundary conditions)
- Each cell: `0` (empty) or `1` (car)
- **Update rule:** Car moves forward if next cell is empty, otherwise stays
- **Velocity metric:** `velocity = (cars that moved) / (total cars)`

### Visual Example
```
Initial:  XX.X...XX..XXXXXX...X.XX.XX.XX
t=1:      XX.X.X.XX.X.XXXX.X.X.X.X.X.XX  (velocity=0.39)
t=15:     XX.X.X.X.X.X.XXXX.X.X.X.X.X.XX (velocity=0.67, stabilized)
```
(X = car, . = empty space)

### Key Phenomena
- **Low density** (Ï<0.3): Free flow, velocityâ‰ˆ1.0
- **Medium density** (0.3<Ï<0.7): Mixed flow, 0.3<velocity<0.7
- **High density** (Ï>0.7): Congestion, velocityâ†’0
- **Traffic jams** form and propagate backwards

## Project Structure

```
HPCReto3/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ ca_serial.c               # âœ… Serial implementation
â”‚   â”œâ”€â”€ ca_mpi_blocking.c         # ğŸ”² MPI with blocking communication
â”‚   â”œâ”€â”€ ca_mpi_nonblocking.c      # ğŸ”² MPI with non-blocking communication
â”‚   â””â”€â”€ ca_mpi_master_worker.c    # ğŸ”² MPI master-worker pattern
â”œâ”€â”€ bin/                          # Compiled binaries
â”‚   â””â”€â”€ ca_serial                 # âœ… Working executable
â”œâ”€â”€ scripts/                      # Automation scripts
â”‚   â”œâ”€â”€ test_serial.sh            # âœ… Test suite (10/10 passed)
â”‚   â”œâ”€â”€ run_benchmarks.sh         # ğŸ”² Benchmark execution (10 runs)
â”‚   â””â”€â”€ analyze_results.py        # ğŸ”² Statistical analysis
â”œâ”€â”€ results/                      # Benchmark outputs
â”‚   â””â”€â”€ benchmarks.csv            # ğŸ”² Raw data
â”œâ”€â”€ docs/                         # Documentation
â”‚   â””â”€â”€ ANALYSIS.md               # âœ… Detailed algorithm analysis
â”œâ”€â”€ Makefile                      # âœ… Build system
â””â”€â”€ README.md                     # This file
```

## Quick Start

### Build
```bash
make              # Build all versions
make ca_serial    # Build only serial version
```

### Run Serial Version
```bash
# Format: ./bin/ca_serial <N> <T> <density> [seed]
./bin/ca_serial 1000 100 0.5 42

# Example outputs:
# N=1000, T=100, density=0.5
#   Mean velocity: 0.9484
#   Time: 0.002 seconds
```

### Test Correctness
```bash
make test-serial
# Runs 10 test cases:
# âœ… Empty road, full road, single car
# âœ… Sparse/medium/dense traffic
# âœ… Large systems, long simulations
# âœ… Determinism, conservation laws
```

## Implementation Status

### âœ… Completed
1. **Analysis** (docs/ANALYSIS.md)
   - Truth tables for CA rules
   - Pseudocode for serial and parallel versions
   - Complexity analysis: O(NÃ—T) serial
   - Verification strategies

2. **Serial Implementation** (src/ca_serial.c)
   - Double buffer update (eliminates race conditions)
   - Periodic boundary conditions
   - Velocity calculation with statistics
   - Conservation verification (checksum)
   - Performance metrics (cell updates/sec)

3. **Testing** (scripts/test_serial.sh)
   - 10/10 tests passed
   - Edge cases: empty, full, single car
   - Phase transitions: sparseâ†’dense traffic
   - Determinism and randomization

### ğŸ”² In Progress
4. **MPI Parallelization Design**
   - 1D domain decomposition
   - Halo exchange (ghost cells)
   - Global reduction for velocity

5. **MPI Implementations**
   - Blocking: `MPI_Sendrecv` for halo exchange
   - Non-blocking: `MPI_Isend`/`MPI_Irecv` + `MPI_Waitall`
   - Master-worker: Dynamic load balancing

6. **Benchmarking**
   - 10 runs per configuration (professor requirement)
   - Variables: N (100Kâ†’1M), T (100â†’1K), P (1,2,4,6)
   - Metrics: execution time, speedup, efficiency

7. **Analysis**
   - Mean Â± std across runs
   - Speedup plots with error bars
   - Strong/weak scaling
   - Communication overhead analysis

## Test Results

### Validation
```
âœ… Conservation:   All tests maintain car count (18 cars â†’ 18 cars)
âœ… Determinism:    Same seed â†’ identical checksums
âœ… Randomization:  Different seeds â†’ different results
âœ… Velocity bounds: Always 0 â‰¤ velocity â‰¤ 1
âœ… Phase behavior:  
   - density=0.3 â†’ velocity=0.992 (free flow)
   - density=0.5 â†’ velocity=0.741 (mixed)
   - density=0.7 â†’ velocity=0.347 (congestion)
```

### Performance (Serial)
```
N=10,000,   T=100:   0.002 sec  (5.0Ã—10â¸ cells/sec)
N=100,000,  T=100:   0.021 sec  (4.7Ã—10â¸ cells/sec)
N=1,000,000, T=100:  0.215 sec  (4.6Ã—10â¸ cells/sec)
```
â†’ Excellent cache locality, linear scaling with N

## Parallelization Strategy

### Domain Decomposition
```
Process 0: [cells 0...N/P-1]     + right ghost cell
Process 1: [cells N/P...2N/P-1]  + left/right ghost cells
...
Process P-1: [cells (P-1)N/P...N-1] + left ghost cell
```

### Communication Pattern
```
Each timestep:
1. Update interior cells (no communication)
2. Exchange boundary cells:
   - Send rightmost cell to right neighbor
   - Send leftmost cell to left neighbor
   - Receive from both neighbors
3. Update boundary cells with ghost data
4. MPI_Reduce: sum moved_count across processes
```

### Expected Speedup
```
Computation: O(N/P)  (embarrassingly parallel)
Communication: O(1)  (constant per process)
Speedup: S(P) â‰ˆ P / (1 + 2/N)  (near-linear for large N)
```

## Cluster Configuration

### AWS Setup (from HPCCasoEstudio3)
```
Node 0 (head):  172.31.28.209  (2 slots)
Node 1:         172.31.18.36   (2 slots)
Node 2:         172.31.16.182  (2 slots)
Total: 6 processes on 3 Ã— t3.micro (1 vCPU, 1GB RAM each)
```

### Hostfile
```
172.31.28.209 slots=2
172.31.18.36 slots=2
172.31.16.182 slots=2
```

### Run on Cluster
```bash
# Serial baseline
./bin/ca_serial 1000000 1000 0.5 42

# MPI versions (when ready)
mpirun --hostfile hostfile -np 6 ./bin/ca_mpi_blocking 1000000 1000 0.5 42
```

## Benchmark Plan

### Parameters
```
N (cells):     100000, 500000, 1000000
T (timesteps): 100, 500, 1000
density:       0.5 (medium traffic)
P (processes): 1, 2, 4, 6
repetitions:   10 (for statistical significance)
```

### Metrics to Collect
- **Execution time**: Mean Â± std across 10 runs
- **Speedup**: S(P) = T(1) / T(P)
- **Efficiency**: E(P) = S(P) / P
- **Coefficient of variation**: CV = std/mean Ã— 100%

### Output Format (CSV)
```csv
version,N,T,density,P,run,execution_time,mean_velocity
serial,100000,100,0.5,1,1,0.0215,0.9034
serial,100000,100,0.5,1,2,0.0218,0.9034
...
mpi_blocking,100000,100,0.5,6,10,0.0041,0.9034
```

## Documentation

### See Also
- **ANALYSIS.md**: Complete algorithm analysis, truth tables, pseudocode
- **MPI_Exercise.txt**: Original challenge statement
- **scripts/test_serial.sh**: Test case definitions

## Next Steps

1. â³ Design MPI communication patterns (halo exchange logic)
2. â³ Implement `ca_mpi_blocking.c` (MPI_Sendrecv)
3. â³ Implement `ca_mpi_nonblocking.c` (MPI_Isend/Irecv)
4. â³ Implement `ca_mpi_master_worker.c` (if time permits)
5. â³ Create `run_benchmarks.sh` (10 repetitions)
6. â³ Create `analyze_results.py` (statistics + plots)
7. â³ Execute benchmarks on AWS cluster
8. â³ Write final report with performance analysis

## References

- Nagel-Schreckenberg model (traffic CA)
- MPI domain decomposition patterns
- Previous project: HPCCasoEstudio3 (matrix multiplication with MPI)

---

**Status:** Serial version complete and tested âœ…  
**Next:** Begin MPI parallelization design
